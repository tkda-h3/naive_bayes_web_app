{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ナイーブベイズ分類器による文書分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prettyprint import pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger('mecabrc')\n",
    "text = \"はじめに こんにちは。システム開発部の中村です。 機械学習についての理解を促進するため、 データから分類モデルを自動で構築する古典的な方法である\"\n",
    "node = mecab.parseToNode(text)\n",
    "# while node:\n",
    "#     if node.feature.split(',')[0] == '名詞':\n",
    "#         print node.surface.lower()\n",
    "#     node = node.next\n",
    "    \n",
    "def get_words_list(text):\n",
    "    mecab = MeCab.Tagger('mecabrc')\n",
    "    node = mecab.parseToNode(text)\n",
    "    words_list = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == '名詞':\n",
    "            words_list.append(node.surface.lower())\n",
    "        node = node.next\n",
    "    return words_list\n",
    "\n",
    "def get_words_matrix(texts_list):\n",
    "    \"\"\"\n",
    "    texts_list : ['text1', 'text2',... ]\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    return [get_words_list(text) for text in texts_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mecabの例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['物語', '女性', 'ファッション', '誌', '編集', '者', 'こと', '女性', '河野', '悦子', '総合', '出版', '社', '景', '凡社', '校閲', '部', '社員', '採用', 'もの', '自分', '場所', 'ここ', '仕事', 'まい進', '校閲', '者', '悦子', '地味', 'スゴイ', '活躍', '物語', 'ため', '丁寧', '導入', '細か', 'リアル', 'さ', '石原', 'さとみ', 'キャスティング', '妙', '校閲', 'ガール', '好評', 'ぶり', 'の', '自動', '運転', '車', '開発', '競争', '世界中', '加熱'], ['オクスフォード', '大学', 'スピンオフ', '自動', '運転', '車', 'スタート', 'アップ', 'oxbotica', '自動', '運転', '技術', '搭載', 'ルノー', 'イギリス', '公道', '実験', '開始', 'イギリス', '公道', '上', '自動', '運転', '車', 'の', 'こと', '場所', 'イングランド', 'ミルトン・キーンズ', '走行', '速度', '時速', '5', 'マイル', 'こと', '試験', '段階', '自動', '運転', '車']]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "物語は、女性ファッション誌の編集者になることを夢見る女性・河野悦子が、総合出版社・景凡社の校閲部の社員として採用されるというもの。「自分のいるべき場所はここじゃない」と思いつつも、仕事にまい進する校閲者・悦子の“地味にスゴイ”活躍を描く。\n",
    "物語に入っていくための丁寧な導入と、細かな“リアルさ”をぶっ飛ばす石原さとみというキャスティングの妙が、『校閲ガール』の好評ぶりにつながっているのだろう。\n",
    "自動運転車の開発競争は世界中で加熱しつつある。\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "オクスフォード大学からのスピンオフにより生まれた自動運転車のスタートアップであるOxboticaが、自動運転技術を搭載したルノーにてイギリスでの公道実験を開始した。イギリス公道上を自動運転車が走るのは初めてのことだ。場所はイングランドのミルトン・キーンズで、走行速度は時速5マイルだとのこと。まだ試験段階ではあるものの、自動運転車は広がりつつある\n",
    "\"\"\"\n",
    "res = get_words_matrix([text1,text2])\n",
    "print str(res).decode('string-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 特徴ベクトル --------\n",
      "[[ 1.  2.  1.  1.  1.  3.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  4.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  4.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  0.  1.  1.  1.  2.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, matutils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(res)\n",
    "\n",
    "\n",
    "bows = dictionary.doc2bow(res[0]) # [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...]\n",
    "bows = [dictionary.doc2bow(x) for x in res]\n",
    "# print '----- BoW -------'\n",
    "#print bows\n",
    "# print \n",
    "train_X = np.array([(matutils.corpus2dense([vec], num_terms=len(dictionary)).T[0]) \n",
    "                   for vec in bows])\n",
    "train_y = np.array([1,0])\n",
    "print '------- 特徴ベクトル --------'\n",
    "print train_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ナイーブベイズ分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearnで動作確認\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomな例（ラベル種類数 : 3、特徴ベクトル次元数 : 40）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_train_X = np.random.randint(4, size=(100,40))#cntは最大4とする\n",
    "random_train_y = np.random.randint(3,size=100) # 正解ラベル数は3\n",
    "random_test_X = np.random.randint(3,size=(20,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 2 1 1 0 1 2 2 0 1 1 1 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(random_train_X,random_train_y)\n",
    "\n",
    "print clf.predict(random_test_X)\n",
    "#print clf.predict_proba(random_test_X)\n",
    "# print clf.predict_log_proba(random_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自分で実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34  0.36  0.3 ]\n",
      "\n",
      "[[2 0 1 0 2 2 1 1 0 0 1 1 1 1 1 0 2 0 0 0 0 1 0 0 1 2 2 0 0 2 0 0 2 2 2 1 1\n",
      "  1 0 0]]\n",
      "\n",
      "[-86.01545709 -86.38800665 -87.0905958 ]\n"
     ]
    }
   ],
   "source": [
    "def calc_cat(y): # calculate P(cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty(label_kinds)\n",
    "    for i in range(label_kinds):\n",
    "        ans[i] = len(np.argwhere(y == i)[:,0]) / float(len(y))\n",
    "    return ans\n",
    "\n",
    "def calc_each_word_bar_cat(X, y): # calculate P(word_id1 | cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty((label_kinds, X.shape[1]))\n",
    "    for i in range(label_kinds):\n",
    "        index = np.argwhere(random_train_y == i)[:,0] # index\n",
    "        ans[i] = (X[index, :].sum(axis=0) + 1).astype(np.float32) / (X[index, :].sum() + X.shape[1]) # lablace smoothing\n",
    "    return ans\n",
    "\n",
    "def calc_log_prob(X, y, word_cat_prob, cat_prob, doc): # doc : 1 feature vector\n",
    "        doc_word_index = np.where(doc != 0)\n",
    "        each_word_prob_in_doc  = word_cat_prob[:, doc_word_index[1]] #docに含まれる単語の各P(word | cat)\n",
    "        each_log_word_cat_prob = np.log(each_word_prob_in_doc).sum(axis=1)\n",
    "        ans = np.log(cat_prob) + each_log_word_cat_prob\n",
    "        return ans\n",
    "\n",
    "cat_prob = calc_cat(random_train_y)\n",
    "print cat_prob\n",
    "print \n",
    "word_cat_prob = calc_each_word_bar_cat(random_train_X, random_train_y)\n",
    "doc = np.random.randint(3,size=(1,40))\n",
    "print doc\n",
    "print\n",
    "print calc_log_prob(random_train_X,random_train_y,word_cat_prob,cat_prob, doc)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 2 1 1 0 1 2 2 0 1 1 1 0 2 1]\n",
      "[0 0 0 0 0 1 2 1 1 0 1 2 0 0 1 1 1 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "cat_prob = calc_cat(random_train_y)\n",
    "word_cat_prob = calc_each_word_bar_cat(random_train_X, random_train_y)\n",
    "res = []\n",
    "docs = random_test_X\n",
    "for i in range(len(docs)):\n",
    "    log_prob = calc_log_prob(random_train_X,random_train_y,word_cat_prob,cat_prob, (docs[i])[np.newaxis,:])\n",
    "    res.append(np.argmax(log_prob))\n",
    "    \n",
    "print clf.predict(random_test_X)\n",
    "print np.array(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
