{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストとラベルの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from prettyprint import pp\n",
    "\n",
    "html = urllib2.urlopen(\"https://gunosy.com/\")\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "def get_categories(url):\n",
    "    html = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > nav > ul > li > a\")[1:-1]\n",
    "    categories = map(lambda a: (a.get(\"href\"), a.string), a_list)\n",
    "    return categories\n",
    "    \n",
    "categories = get_categories(\"https://gunosy.com/\")\n",
    "\n",
    "def get_links_and_titles(category_url):\n",
    "    html = urllib2.urlopen(category_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > div > div > div.main > div.article_list.gtm-click > div.list_content > div.list_text > div.list_title > a\")\n",
    "    links = map(lambda a: a.get(\"href\"), a_list)\n",
    "    titles = map(lambda a: a.string.encode('utf-8'), a_list)\n",
    "    return links,titles\n",
    "\n",
    "\n",
    "all_titles = []\n",
    "all_links = []\n",
    "all_labels = []\n",
    "for i,url_info in enumerate(categories):\n",
    "    links,titles = get_links_and_titles(url_info[0])\n",
    "    all_links.extend(links)\n",
    "    all_titles.extend(titles)\n",
    "    all_labels.extend([i] * len(links))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "def get_words_list(text):\n",
    "    mecab = MeCab.Tagger('mecabrc')\n",
    "    node = mecab.parseToNode(text)\n",
    "    words_list = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == '名詞':\n",
    "            words_list.append(node.surface.lower())\n",
    "        node = node.next\n",
    "    return words_list\n",
    "\n",
    "def get_words_matrix(texts_list):\n",
    "    \"\"\"\n",
    "    texts_list : ['text1', 'text2',... ]\n",
    "    \"\"\"\n",
    "    return [get_words_list(text) for text in texts_list]\n",
    "\n",
    "\n",
    "from gensim import corpora, matutils\n",
    "import numpy as np\n",
    "\n",
    "res = get_words_matrix(all_titles)\n",
    "dictionary = corpora.Dictionary(res)\n",
    "bows = dictionary.doc2bow(res[0]) # [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...]\n",
    "bows = [dictionary.doc2bow(x) for x in res]\n",
    "\n",
    "X = np.array([(matutils.corpus2dense([vec], num_terms=len(dictionary)).T[0]) \n",
    "                   for vec in bows])\n",
    "y = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練データとテストデータに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 番外編：sklearn.naive_bayes で動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 正解ラベル ----\n",
      "[6 7 5 4 1 4 4 6 1 1 5 2 1 4 3 4 3 6 0 2 3 1 1 1 0 3 1 7 2 3 7 7 7 6 7 0 7\n",
      " 0 0 3 1 4 0 4 4 5 2 5 7 5 0 0 2]\n",
      "----- 予測ラベル ----\n",
      "[3 7 5 4 2 4 4 2 7 1 5 0 0 4 3 4 3 3 7 2 0 1 1 1 1 1 1 1 2 3 4 7 1 6 1 0 5\n",
      " 2 2 0 2 5 1 4 1 2 5 3 7 5 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47169811320754718"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearnで動作確認\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(train_X,train_y)\n",
    "print '----- 正解ラベル ----'\n",
    "print test_y\n",
    "print '----- 予測ラベル ----'\n",
    "print clf.predict(test_X)\n",
    "# print np.argmax(clf.predict_log_proba(test_X), axis=1)\n",
    "clf.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ナイーブベイズによる分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cat(y): # calculate P(cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty(label_kinds)\n",
    "    for i in range(label_kinds):\n",
    "        ans[i] = len(np.argwhere(y == i)[:,0]) / float(len(y))\n",
    "    return ans\n",
    "\n",
    "def calc_each_word_bar_cat(X, y): # calculate P(word_id1 | cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty((label_kinds, X.shape[1]))\n",
    "    for i in range(label_kinds):\n",
    "        index = np.argwhere(y == i)[:,0] # index\n",
    "        ans[i] = (X[index, :].sum(axis=0) + 1).astype(np.float32) / (X[index, :].sum() + X.shape[1]) # lablace smoothing\n",
    "    return ans\n",
    "\n",
    "def calc_log_prob(X, y, word_cat_prob, cat_prob, doc): # doc : 1 feature vector\n",
    "        doc_word_index = np.where(doc != 0)\n",
    "        each_word_prob_in_doc  = word_cat_prob[:, doc_word_index[1]] #docに含まれる単語の各P(word | cat)\n",
    "        each_log_word_cat_prob = np.log(each_word_prob_in_doc).sum(axis=1)\n",
    "        ans = np.log(cat_prob) + each_log_word_cat_prob\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能評価 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- sklearn -----\n",
      "0.466897776137\n",
      "---- my prediction -----\n",
      "0.441883116883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_prob = calc_cat(train_y)\n",
    "word_cat_prob = calc_each_word_bar_cat(train_X, train_y)\n",
    "pred_y = []\n",
    "docs = test_X\n",
    "for i in range(len(docs)):\n",
    "    log_prob = calc_log_prob(train_X,train_y,word_cat_prob,cat_prob, (docs[i])[np.newaxis,:])\n",
    "    pred_y.append(np.argmax(log_prob))\n",
    "pred_y = np.array(pred_y)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sk_pred_y =  clf.predict(test_X)\n",
    "print '---- sklearn -----'\n",
    "print f1_score(test_y, sk_pred_y, average='macro')\n",
    "print '---- my prediction -----'\n",
    "print f1_score(test_y, pred_y, average='macro')\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
