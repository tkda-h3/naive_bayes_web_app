{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストとラベルの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gunosy.com/categories/1 8\n",
      "https://gunosy.com/categories/2 8\n",
      "https://gunosy.com/categories/3 8\n",
      "https://gunosy.com/categories/4 8\n",
      "https://gunosy.com/categories/5 8\n",
      "https://gunosy.com/categories/6 8\n",
      "https://gunosy.com/categories/7 8\n",
      "https://gunosy.com/categories/8 8\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from prettyprint import pp\n",
    "\n",
    "def get_categories(url):\n",
    "    html = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > nav > ul > li > a\")[1:-1]\n",
    "    categories = map(lambda a: (a.get(\"href\"), a.string), a_list)\n",
    "    return categories\n",
    "    \n",
    "categories = get_categories(\"https://gunosy.com/\")\n",
    "\n",
    "def get_content(page_url):\n",
    "    html = urllib2.urlopen(page_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    content_tag = soup.select(\"body div.main.article_main > div.article.gtm-click\")\n",
    "    return ('').join( [text_tag.get_text() for text_tag in content_tag] ).encode('utf-8')\n",
    "    \n",
    "def get_links_and_contents(category_url):\n",
    "    html = urllib2.urlopen(category_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > div > div > div.main > div.article_list.gtm-click > div.list_content > div.list_text > div.list_title > a\")\n",
    "    links = map(lambda a: a.get(\"href\"), a_list)\n",
    "    contents = map(lambda url: get_content(url), links)\n",
    "    return links,contents\n",
    "\n",
    "all_contents = []\n",
    "all_links = []\n",
    "all_labels = []\n",
    "for i,url_info in enumerate(categories):\n",
    "    for page_num in xrange(1,9):\n",
    "        if page_num % 8 == 0: print url_info[0], page_num\n",
    "        pager_query = '?page=%d' % page_num\n",
    "        url = url_info[0] + pager_query\n",
    "        links,contents = get_links_and_contents(url)\n",
    "        all_links.extend(links)\n",
    "        all_contents.extend(contents)\n",
    "        all_labels.extend([i] * len(links))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数 1280\n",
      "特徴ベクトルの次元数 6401\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "def get_words_list(text):\n",
    "    mecab = MeCab.Tagger('mecabrc')\n",
    "    node = mecab.parseToNode(text)\n",
    "    words_list = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == '名詞':\n",
    "            words_list.append(node.surface.lower())\n",
    "        node = node.next\n",
    "    return words_list\n",
    "\n",
    "def get_words_matrix(texts_list):\n",
    "    \"\"\"\n",
    "    texts_list : ['text1', 'text2',... ]\n",
    "    \"\"\"\n",
    "    return [get_words_list(text) for text in texts_list]\n",
    "\n",
    "\n",
    "from gensim import corpora, matutils\n",
    "import numpy as np\n",
    "\n",
    "res = get_words_matrix(all_contents)\n",
    "dictionary = corpora.Dictionary(res)\n",
    "dictionary.filter_extremes(no_below=4 , no_above=0.2)\n",
    "\n",
    "# [ [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] ,\n",
    "#   [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] ,\n",
    "#]\n",
    "bows = [dictionary.doc2bow(x) for x in res]\n",
    "\n",
    "X = np.array([(matutils.corpus2dense([vec], num_terms=len(dictionary)).T[0]) \n",
    "                   for vec in bows])\n",
    "y = np.array(all_labels)\n",
    "\n",
    "print 'データ数', X.shape[0]\n",
    "print '特徴ベクトルの次元数', X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ナイーブベイズによる分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cat(y): # calculate P(cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty(label_kinds)\n",
    "    for i in range(label_kinds):\n",
    "        ans[i] = len(np.argwhere(y == i)[:,0]) / float(len(y))\n",
    "    return ans\n",
    "\n",
    "def calc_each_word_bar_cat(X, y): # calculate P(word_id1 | cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty((label_kinds, X.shape[1]))\n",
    "    for i in range(label_kinds):\n",
    "        index = np.argwhere(y == i)[:,0] # index\n",
    "        ans[i] = (X[index, :].sum(axis=0) + 1).astype(np.float32) / (X[index, :].sum() + X.shape[1]) # lablace smoothing\n",
    "    return ans\n",
    "\n",
    "def calc_log_prob(X, y, word_cat_prob, cat_prob, doc): # doc : 1 feature vector\n",
    "        doc_word_index = np.where(doc != 0)\n",
    "        each_word_prob_in_doc  = word_cat_prob[:, doc_word_index[1]] #docに含まれる単語の各P(word | cat)\n",
    "        each_log_word_cat_prob = np.log(each_word_prob_in_doc).sum(axis=1)\n",
    "        ans = np.log(cat_prob) + each_log_word_cat_prob\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能評価を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(train_X,test_X,train_y,test_y):\n",
    "    cat_prob = calc_cat(train_y)\n",
    "    word_cat_prob = calc_each_word_bar_cat(train_X, train_y)\n",
    "    pred_y = []\n",
    "    for vec in test_X:\n",
    "        log_prob = calc_log_prob(train_X,train_y,word_cat_prob,cat_prob, (vec)[np.newaxis,:])\n",
    "        pred_y.append(np.argmax(log_prob))\n",
    "    pred_y = np.array(pred_y)\n",
    "    \n",
    "    score = f1_score(test_y, pred_y, average='macro')\n",
    "    return score\n",
    "    \n",
    "#evaluate_model(train_X,test_X,train_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation で評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8436964   0.83319539  0.83514365  0.87935666  0.83902993]\n",
      "結果 0.846084408705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# evaluate_model(train_X,test_X,train_y,test_y)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True, random_state = 6789)\n",
    "scores = []\n",
    "for train_index, test_index in skf:\n",
    "    #print train_index, test_index\n",
    "    train_X, test_X = X[train_index], X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]  \n",
    "    score = evaluate_model(train_X,test_X,train_y,test_y)\n",
    "    scores.append(score)\n",
    "scores = np.array(scores)\n",
    "print scores\n",
    "print '結果' ,np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
