{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストとラベルの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gunosy.com/categories/1 5\n",
      "https://gunosy.com/categories/2 5\n",
      "https://gunosy.com/categories/3 5\n",
      "https://gunosy.com/categories/4 5\n",
      "https://gunosy.com/categories/5 5\n",
      "https://gunosy.com/categories/6 5\n",
      "https://gunosy.com/categories/7 5\n",
      "https://gunosy.com/categories/8 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from prettyprint import pp\n",
    "\n",
    "def get_categories(url):\n",
    "    html = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > nav > ul > li > a\")[1:-1]\n",
    "    categories = map(lambda a: (a.get(\"href\"), a.string), a_list)\n",
    "    return categories\n",
    "    \n",
    "categories = get_categories(\"https://gunosy.com/\")\n",
    "\n",
    "def get_content(page_url):\n",
    "    html = urllib2.urlopen(page_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    content_tag = soup.select(\"body div.main.article_main > div.article.gtm-click\")\n",
    "    return ('').join( [text_tag.get_text() for text_tag in content_tag] ).encode('utf-8')\n",
    "    \n",
    "def get_links_and_contents(category_url):\n",
    "    html = urllib2.urlopen(category_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > div > div > div.main > div.article_list.gtm-click > div.list_content > div.list_text > div.list_title > a\")\n",
    "    links = map(lambda a: a.get(\"href\"), a_list)\n",
    "    contents = map(lambda url: get_content(url), links)\n",
    "    return links,contents\n",
    "\n",
    "all_contents = []\n",
    "all_links = []\n",
    "all_labels = []\n",
    "for i,url_info in enumerate(categories):\n",
    "    for page_num in xrange(1,6):\n",
    "        if page_num % 5 == 0: print url_info[0], page_num\n",
    "        pager_query = '?page=%d' % page_num\n",
    "        url = url_info[0] + pager_query\n",
    "        links,contents = get_links_and_contents(url)\n",
    "        all_links.extend(links)\n",
    "        all_contents.extend(contents)\n",
    "        all_labels.extend([i] * len(links))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数 800\n",
      "特徴ベクトルの次元数 4741\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "def get_words_list(text):\n",
    "    mecab = MeCab.Tagger('mecabrc')\n",
    "    node = mecab.parseToNode(text)\n",
    "    words_list = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == '名詞':\n",
    "            words_list.append(node.surface.lower())\n",
    "        node = node.next\n",
    "    return words_list\n",
    "\n",
    "def get_words_matrix(texts_list):\n",
    "    \"\"\"\n",
    "    texts_list : ['text1', 'text2',... ]\n",
    "    \"\"\"\n",
    "    return [get_words_list(text) for text in texts_list]\n",
    "\n",
    "\n",
    "from gensim import corpora, matutils\n",
    "import numpy as np\n",
    "\n",
    "res = get_words_matrix(all_contents)\n",
    "dictionary = corpora.Dictionary(res)\n",
    "dictionary.filter_extremes(no_below=4 , no_above=0.2)\n",
    "\n",
    "# [ [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] ,\n",
    "#   [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] ,\n",
    "#]\n",
    "bows = [dictionary.doc2bow(x) for x in res]\n",
    "\n",
    "X = np.array([(matutils.corpus2dense([vec], num_terms=len(dictionary)).T[0]) \n",
    "                   for vec in bows])\n",
    "y = np.array(all_labels)\n",
    "\n",
    "print 'データ数', X.shape[0]\n",
    "print '特徴ベクトルの次元数', X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ナイーブベイズによる分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cat(y): # calculate P(cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty(label_kinds)\n",
    "    for i in range(label_kinds):\n",
    "        ans[i] = len(np.argwhere(y == i)[:,0]) / float(len(y))\n",
    "    return ans\n",
    "\n",
    "def calc_each_word_bar_cat(X, y): # calculate P(word_id1 | cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty((label_kinds, X.shape[1]))\n",
    "    for i in range(label_kinds):\n",
    "        index = np.argwhere(y == i)[:,0] # index\n",
    "        ans[i] = (X[index, :].sum(axis=0) + 1).astype(np.float32) / (X[index, :].sum() + X.shape[1]) # laplace smoothing\n",
    "    return ans\n",
    "\n",
    "def calc_log_prob(word_cat_prob, cat_prob, doc): # doc : 1 feature vector\n",
    "        doc_word_index = np.where(doc != 0)\n",
    "        each_word_prob_in_doc  = word_cat_prob[:, doc_word_index[1]] #docに含まれる単語の各P(word | cat)\n",
    "        each_log_word_cat_prob = np.log(each_word_prob_in_doc).sum(axis=1)\n",
    "        ans = np.log(cat_prob) + each_log_word_cat_prob\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能評価を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(train_X,test_X,train_y,test_y):\n",
    "    cat_prob = calc_cat(train_y)\n",
    "    word_cat_prob = calc_each_word_bar_cat(train_X, train_y)\n",
    "    pred_y = []\n",
    "    for vec in test_X:\n",
    "        log_prob = calc_log_prob(word_cat_prob,cat_prob, (vec)[np.newaxis,:])\n",
    "        pred_y.append(np.argmax(log_prob))\n",
    "    pred_y = np.array(pred_y)\n",
    "    \n",
    "    score = f1_score(test_y, pred_y, average='macro')\n",
    "    return score\n",
    "    \n",
    "#evaluate_model(train_X,test_X,train_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation で評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8737455   0.89922887  0.8276687   0.8756959   0.81797754]\n",
      "結果 0.858863300311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# evaluate_model(train_X,test_X,train_y,test_y)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True, random_state = 6789)\n",
    "scores = []\n",
    "for train_index, test_index in skf:\n",
    "    #print train_index, test_index\n",
    "    train_X, test_X = X[train_index], X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]  \n",
    "    score = evaluate_model(train_X,test_X,train_y,test_y)\n",
    "    scores.append(score)\n",
    "scores = np.array(scores)\n",
    "print scores\n",
    "print '結果' ,np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力フォームでURLを受け取り→カテゴリ名予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前に保持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": \"エンタメ\", \n",
      "    \"1\": \"スポーツ\", \n",
      "    \"2\": \"おもしろ\", \n",
      "    \"3\": \"国内\", \n",
      "    \"4\": \"海外\", \n",
      "    \"5\": \"コラム\", \n",
      "    \"6\": \"IT・科学\", \n",
      "    \"7\": \"グルメ\"\n",
      "}\n",
      "------ Save completed. ------\n"
     ]
    }
   ],
   "source": [
    "cat_prob = calc_cat(y)\n",
    "word_cat_prob = calc_each_word_bar_cat(X,y)\n",
    "dictionary\n",
    "cvt_y_to_category_name = dict(map(lambda (i,x): (i,x[1].encode('utf-8')), enumerate(categories)))#ラベルy から　カテゴリ名に変換するdict\n",
    "pp(cvt_y_to_category_name)\n",
    "\n",
    "SAVE_FLAG = True\n",
    "if SAVE_FLAG:\n",
    "    import pickle\n",
    "    def save_as_pickle(file_path, obj):\n",
    "        with open(file_path, 'w') as f:\n",
    "            pickle.dump(obj, f)    \n",
    "    save_as_pickle('dictionay.dump', dictionary)\n",
    "    save_as_pickle('cat_prob.dump', cat_prob)\n",
    "    save_as_pickle('word_cat_prob.dump', word_cat_prob)\n",
    "    save_as_pickle('cvt_y_to_category_name.dump', cvt_y_to_category_name)\n",
    "    print '------ Save completed. ------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリ名予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----　入力を受け取ります ------\n",
      "入力されたURL : https://gunosy.com/articles/at0SJ\n",
      "----------　予測結果 ------------\n",
      "予測カテゴリ : おもしろ\n",
      "\n",
      "0.844357013702 [sec]\n"
     ]
    }
   ],
   "source": [
    "def get_obj_from_pickle(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def predict_y_of_input_url(input_url, dictionary,word_cat_prob, cat_prob):\n",
    "    text_of_input_url = get_content(input_url)\n",
    "    word_list_of_input_url = get_words_list(text_of_input_url)\n",
    "    bow_of_input_url = dictionary.doc2bow(word_list_of_input_url) #[(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] \n",
    "    test_X = np.array([matutils.corpus2dense([bow_of_input_url], num_terms=len(dictionary)).T[0]])\n",
    "    log_prob = calc_log_prob(word_cat_prob,cat_prob, test_X)\n",
    "    return np.argmax(log_prob)\n",
    "    \n",
    "\n",
    "# deserialize\n",
    "dir_path = './'\n",
    "c_prob = get_obj_from_pickle(dir_path+'cat_prob.dump')\n",
    "w_c_prob = get_obj_from_pickle(dir_path+'word_cat_prob.dump')\n",
    "cvt_y_to_c_name = get_obj_from_pickle(dir_path+'cvt_y_to_category_name.dump')\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "print '-----　入力を受け取ります ------'\n",
    "input_url = 'https://gunosy.com/articles/at0SJ' # 例\n",
    "print '入力されたURL :', input_url\n",
    "pred_y_of_input_url = predict_y_of_input_url(input_url, dictionary,w_c_prob, c_prob)\n",
    "pred_category_name_of_input_url = cvt_y_to_c_name[pred_y_of_input_url] #数字ラベルを変換\n",
    "print '----------　予測結果 ------------'\n",
    "print '予測カテゴリ :', pred_category_name_of_input_url\n",
    "print\n",
    "print time.time()-st, '[sec]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
