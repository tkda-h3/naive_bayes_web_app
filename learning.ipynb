{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストとラベルの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from prettyprint import pp\n",
    "\n",
    "html = urllib2.urlopen(\"https://gunosy.com/\")\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "def get_categories(url):\n",
    "    html = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > nav > ul > li > a\")[1:-1]\n",
    "    categories = map(lambda a: (a.get(\"href\"), a.string), a_list)\n",
    "    return categories\n",
    "    \n",
    "categories = get_categories(\"https://gunosy.com/\")\n",
    "\n",
    "def get_content(page_url):\n",
    "    html = urllib2.urlopen(page_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    content_tag = soup.select(\"body div.main.article_main > div.article.gtm-click\")\n",
    "    return ('').join( [text_tag.get_text() for text_tag in content_tag] ).encode('utf-8')\n",
    "    \n",
    "def get_links_and_contents(category_url):\n",
    "    html = urllib2.urlopen(category_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    a_list = soup.select(\"body > div > div > div.main > div.article_list.gtm-click > div.list_content > div.list_text > div.list_title > a\")\n",
    "    links = map(lambda a: a.get(\"href\"), a_list)\n",
    "    contents = map(lambda url: get_content(url), links)\n",
    "    return links,contents\n",
    "\n",
    "all_contents = []\n",
    "all_links = []\n",
    "all_labels = []\n",
    "for i,url_info in enumerate(categories):\n",
    "    for page_num in xrange(1,13):\n",
    "        if page_num % 10 == 0: print url_info[0], page_num\n",
    "        pager_query = '?page=%d' % page_num\n",
    "        url = url_info[0] + pager_query\n",
    "        links,contents = get_links_and_contents(url)\n",
    "        all_links.extend(links)\n",
    "        all_contents.extend(contents)\n",
    "        all_labels.extend([i] * len(links))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "def get_words_list(text):\n",
    "    mecab = MeCab.Tagger('mecabrc')\n",
    "    node = mecab.parseToNode(text)\n",
    "    words_list = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == '名詞':\n",
    "            words_list.append(node.surface.lower())\n",
    "        node = node.next\n",
    "    return words_list\n",
    "\n",
    "def get_words_matrix(texts_list):\n",
    "    \"\"\"\n",
    "    texts_list : ['text1', 'text2',... ]\n",
    "    \"\"\"\n",
    "    return [get_words_list(text) for text in texts_list]\n",
    "\n",
    "\n",
    "from gensim import corpora, matutils\n",
    "import numpy as np\n",
    "\n",
    "res = get_words_matrix(all_contents)\n",
    "dictionary = corpora.Dictionary(res)\n",
    "dictionary.filter_extremes(no_below=4 , no_above=0.2)\n",
    "\n",
    "# [ [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] ,\n",
    "#   [(w_id1, w_id1_cnt), (w_id2, w_id2_cnt),...] ,\n",
    "#]\n",
    "bows = [dictionary.doc2bow(x) for x in res]\n",
    "\n",
    "\n",
    "X = np.array([(matutils.corpus2dense([vec], num_terms=len(dictionary)).T[0]) \n",
    "                   for vec in bows])\n",
    "y = np.array(all_labels)\n",
    "\n",
    "print 'データ数', X.shape[0]\n",
    "print '特徴ベクトルの次元数', X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練データとテストデータに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.333333, random_state=12345)\n",
    "# train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.333333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ナイーブベイズによる分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cat(y): # calculate P(cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty(label_kinds)\n",
    "    for i in range(label_kinds):\n",
    "        ans[i] = len(np.argwhere(y == i)[:,0]) / float(len(y))\n",
    "    return ans\n",
    "\n",
    "def calc_each_word_bar_cat(X, y): # calculate P(word_id1 | cat)\n",
    "    label_kinds = len(np.unique(y))\n",
    "    ans = np.empty((label_kinds, X.shape[1]))\n",
    "    for i in range(label_kinds):\n",
    "        index = np.argwhere(y == i)[:,0] # index\n",
    "        ans[i] = (X[index, :].sum(axis=0) + 1).astype(np.float32) / (X[index, :].sum() + X.shape[1]) # lablace smoothing\n",
    "    return ans\n",
    "\n",
    "def calc_log_prob(X, y, word_cat_prob, cat_prob, doc): # doc : 1 feature vector\n",
    "        doc_word_index = np.where(doc != 0)\n",
    "        each_word_prob_in_doc  = word_cat_prob[:, doc_word_index[1]] #docに含まれる単語の各P(word | cat)\n",
    "        each_log_word_cat_prob = np.log(each_word_prob_in_doc).sum(axis=1)\n",
    "        ans = np.log(cat_prob) + each_log_word_cat_prob\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能評価 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_prob = calc_cat(train_y)\n",
    "word_cat_prob = calc_each_word_bar_cat(train_X, train_y)\n",
    "pred_y = []\n",
    "docs = test_X\n",
    "for i in range(len(docs)):\n",
    "    log_prob = calc_log_prob(train_X,train_y,word_cat_prob,cat_prob, (docs[i])[np.newaxis,:])\n",
    "    pred_y.append(np.argmax(log_prob))\n",
    "pred_y = np.array(pred_y)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print '---- my prediction -----'\n",
    "print f1_score(test_y, pred_y, average='macro')\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
